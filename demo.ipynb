{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "from keras.applications import*\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.utils import multi_gpu_model\n",
    "import pickle\n",
    "import keras.backend as K\n",
    "from utils import *\n",
    "import os, sys\n",
    "from skll.metrics import spearman\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data number: 10125\n"
     ]
    }
   ],
   "source": [
    "data_dir='kadid10k/images/'\n",
    "input_shape = (512, 512, 3)\n",
    "batch_size = 128\n",
    "data_ids = pd.read_csv('kadid10k/dmos.csv')\n",
    "print('data number:',len(data_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Extract MLSP feature from pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 images\n",
      "2000 images\n",
      "3000 images\n",
      "4000 images\n",
      "5000 images\n",
      "6000 images\n",
      "7000 images\n",
      "8000 images\n",
      "9000 images\n",
      "10000 images\n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "model = model_inceptionresnet_multigap()\n",
    "data_feats = extract_mlsp_feats(data_ids,model,data_dir)\n",
    "data_dmos = data_ids[['dmos']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train a regression model for IQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "srocc: 0.9366366703318774\n",
      "2\n",
      "srocc: 0.9251692421223607\n",
      "3\n",
      "srocc: 0.9256040742552997\n",
      "4\n",
      "srocc: 0.9207364475284746\n",
      "5\n",
      "srocc: 0.9420689617354758\n"
     ]
    }
   ],
   "source": [
    "iter_num = 5\n",
    "srocc_result = []\n",
    "\n",
    "for i in range(iter_num):\n",
    "    print(str(i+1))\n",
    "    data = sample_data(filepath='kadid10k/dmos.csv',feats=data_feats,dmos=data_dmos)\n",
    "    model = fc_model()\n",
    "    model.compile(loss=keras.losses.mean_squared_error,\n",
    "                  optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, \n",
    "                                                  epsilon=None, decay=0.0,amsgrad=False))\n",
    "    # checkpoint\n",
    "    filepath ='model/multigap_best_model.hdf5'\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                                 monitor='val_loss', \n",
    "                                                 verbose=0, \n",
    "                                                 save_best_only=True, \n",
    "                                                 mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    history = model.fit(data['train_feats'],data['train_dmos'],\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=30,\n",
    "                        verbose=0,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_data=(data['valid_feats'],data['valid_dmos']))\n",
    "    model.load_weights(filepath)\n",
    "    y_pred = model.predict(data['test_feats'], batch_size=batch_size)\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    test_srocc = spearman(data['test_dmos'],y_pred)\n",
    "    print('srocc:', str(test_srocc))\n",
    "    srocc_result.append(test_srocc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
